{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained Megatron GPT2 model uses a sequence length of 1024 and an embedding size of 1280. It contains 36 transformer decoder blocks and has 20 attention heads for each attention layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained Megatron GPT2 checkpoints\n",
    "!mkdir -p /content/nvidia/megatron-gpt2-345m\n",
    "!wget --content-disposition https://moyix.net/~moyix/csrc_final.zip -O /content/nvidia/megatron-gpt2-345m/checkpoint.zip\n",
    "%cd /content/nvidia/megatron-gpt2-345m\n",
    "!unzip checkpoint.zip\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this model using HuggingFace Transfoemers, we first need to convert the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ShumengJ/transformers.git /content/transformers\n",
    "!pip install transformers\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/transformers/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py /content/nvidia/megatron-gpt2-345m/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the model using `from_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from $MYDIR/nvidia/megatron-gpt2-345m\n",
    "directory = '/content/nvidia/megatron-gpt2-345m'\n",
    "model = GPT2LMHeadModel.from_pretrained(directory)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "print('** Model loaded **')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During fine-tuning, we only updated the parameters in the last decoder and the output linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.35.ln_1.weight\n",
      "transformer.h.35.ln_1.bias\n",
      "transformer.h.35.attn.c_attn.weight\n",
      "transformer.h.35.attn.c_attn.bias\n",
      "transformer.h.35.attn.c_proj.weight\n",
      "transformer.h.35.attn.c_proj.bias\n",
      "transformer.h.35.ln_2.weight\n",
      "transformer.h.35.ln_2.bias\n",
      "transformer.h.35.mlp.c_fc.weight\n",
      "transformer.h.35.mlp.c_fc.bias\n",
      "transformer.h.35.mlp.c_proj.weight\n",
      "transformer.h.35.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "# freeze parameters in first 35 decoders and print the rest trainable layers\n",
    "i = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if i < 422:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model uses byte pair encoding, the pre-trained tokenizer contains 52001 different tokens including special tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Megatron was trained with standard tokenizer(s).\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    vocab_file='/home/sj3233/project21S/nvidia/megatron-gpt2-345m/vocab.json',\n",
    "    merges_file='/home/sj3233/project21S/nvidia/megatron-gpt2-345m/merges.txt')\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"additional_special_tokens\": [\"<to-buggy>\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add one additional special token: `<to-buggy>: 52001` as a delimiter between the patched code and buggy code, so the size of vocabulary is 52002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is a `.csv` file that contains 225202 datapoints, each datapoint has four columns. The first column is bug `id`, with this id we can verify the raw commit message where this bug came from. Then we saved diff of the code in three columns, `[before]` has the codes that appear before the bugs and remain unchanged during fix. `[buggy_code]` and `[patched_code]` contain the bugs and patches respectively. The reason why we save data like this is to reduce the size of the file so the complete bugs can be obtained by concatenating `[before]` and `[buggy_code]`, the complete patches can be obtained by concatenating `[before]` and `[patched_code]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225202\n"
     ]
    }
   ],
   "source": [
    "path = '/scratch/sj3233/result1000.csv'\n",
    "df = pd.read_csv(path, sep='@')\n",
    "print(len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input sequence (tps) is composed of the following parts: `<s> + [patched_code] + <to-buggy> + [buggy_code] + </s>`. \n",
    "All the sequences are padded to the same maximum length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuggyPatchDataset(Dataset):\n",
    "  def __init__(self, buggy_code, patched_code, before, tokenizer, max_len):\n",
    "    self.buggy_code = buggy_code\n",
    "    self.patched_code = patched_code\n",
    "    self.before = before\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.buggy_code)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    buggy = str(self.buggy_code[item])\n",
    "    patched = str(self.patched_code[item])\n",
    "    before = str(self.before[item])\n",
    "    tps = '<s>' + before + patched + '<to-buggy>' + before + buggy + '</s>'\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        tps,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    encoding_p = self.tokenizer.encode_plus(\n",
    "        '<s>' + before + patched + '<to-buggy>',\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    encoding_b = self.tokenizer.encode_plus(\n",
    "        before + buggy + '</s>',\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_tps': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'patch_ids': encoding_p['input_ids'].flatten(),\n",
    "        'buggy_ids': encoding_b['input_ids'].flatten()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset: 80% training, 10% validation, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180161, 4)\n",
      "(22520, 4)\n",
      "(22521, 4)\n"
     ]
    }
   ],
   "source": [
    "df_train, val_test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "df_val, df_test = train_test_split(val_test, train_size=0.5, random_state=42)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat a dataloader to load the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = BuggyPatchDataset(\n",
    "      buggy_code=df.buggy_code.to_numpy(),\n",
    "      patched_code=df.patched_code.to_numpy(),\n",
    "      before=df.before.to_numpy(),\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=2\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = creat_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = creat_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = creat_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_tps', 'attention_mask', 'patch_ids', 'buggy_ids'])\n",
      "torch.Size([8, 1024])\n",
      "torch.Size([8, 1024])\n",
      "torch.Size([8, 1024])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(val_data_loader))\n",
    "# verify the size of input and datatype\n",
    "print(data.keys())\n",
    "print(data['input_tps'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['buggy_ids'].shape)\n",
    "print(data['buggy_ids'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the token with highest probability from the output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_ids(out_logits):\n",
    "    pred_ids = torch.argmax(out_logits, dim=2)\n",
    "    return pred_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask all the tokens before the delimiter so thaat they would be ignored when calculating loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mask(input_tps):\n",
    "    tps_np = input_tps.cpu().numpy()\n",
    "    delim_index = np.argwhere(tps_np==52001)\n",
    "    for i in range(len(delim_index)):\n",
    "        tps_np[i,0:delim_index[i,1]] = -100\n",
    "        \n",
    "    return torch.from_numpy(tps_np) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor BLEU score for up to 4-grams using uniform weights (called BLEU-4) for both training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_bleu(labels, output_ids, tokenizer):\n",
    "    labels_np = labels[..., 1:].cpu().numpy()\n",
    "    output_np = output_ids[..., :-1].cpu().numpy()\n",
    "    tokenizer = tokenizer\n",
    "    \n",
    "    delim_index = np.argwhere(labels_np==52001)\n",
    "    \n",
    "    expect_out = []\n",
    "    actual_out = []\n",
    "    \n",
    "    for i in range(len(delim_index)):\n",
    "        expect_out = expect_out + labels_np[i,delim_index[i,1]:].tolist()\n",
    "        actual_out = actual_out + output_np[i,delim_index[i,1]:].tolist()\n",
    "    \n",
    "    expect_out = np.delete(np.array(expect_out), np.argwhere(np.array(expect_out)==1))\n",
    "    actual_out = np.delete(np.array(actual_out), np.argwhere(np.array(actual_out)==1))\n",
    "        \n",
    "    expect_out = torch.from_numpy(expect_out)\n",
    "    actual_out = torch.from_numpy(actual_out)\n",
    "\n",
    "    label_token = tokenizer.convert_ids_to_tokens(expect_out)\n",
    "    output_token = tokenizer.convert_ids_to_tokens(actual_out)\n",
    "\n",
    "    smooth = SmoothingFunction()\n",
    "    bleu_model = sentence_bleu([label_token], output_token, smoothing_function=smooth.method1)\n",
    "\n",
    "    return bleu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_data_loader, device):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_bleu = 0\n",
    "  batch_num = 1\n",
    "\n",
    "  for batch in val_data_loader:\n",
    "\n",
    "    input_tps = batch['input_tps'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = label_mask(input_tps).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tps, attention_mask=attention_mask, labels=labels)\n",
    "    output_ids = get_output_ids(outputs.logits)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss\n",
    "    \n",
    "    bleu_model = compute_metrics_bleu(labels, output_ids, tokenizer)\n",
    "    total_bleu += bleu_model\n",
    "    \n",
    "    print(\"\\r evaluating...{:d}/{:d} \".format(batch_num, len(val_data_loader)), end='',  flush=True)\n",
    "    batch_num = batch_num + 1\n",
    "  \n",
    "  val_loss = total_loss/len(val_data_loader)\n",
    "  val_bleu = total_bleu/len(val_data_loader)\n",
    "  \n",
    "  print('\\neval loss:',val_loss.item())\n",
    "  print('eval bleu:',val_bleu)\n",
    "\n",
    "  return val_loss.item(), val_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = defaultdict(list)\n",
    "min_loss = 100\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=2e-5,  \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-08,\n",
    "    weight_decay=0.01)  \n",
    "\n",
    "print('** Start **')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  total_loss = 0\n",
    "  total_bleu = 0\n",
    "  batch_num = 1\n",
    "  print('EPOCH: %d/%d '%(epoch+1, EPOCHS))\n",
    "\n",
    "  for batch in train_data_loader:\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    input_tps = batch['input_tps'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = label_mask(input_tps).to(device)\n",
    "\n",
    "    outputs = model(input_tps, attention_mask=attention_mask, labels=labels)\n",
    "    output_ids = get_output_ids(outputs.logits)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    total_loss += loss\n",
    "    \n",
    "    bleu_model = compute_metrics_bleu(labels, output_ids, tokenizer)\n",
    "    total_bleu += bleu_model\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    print(\"\\r training...{:d}/{:d} \".format(batch_num, len(train_data_loader)), end='',  flush=True)\n",
    "    batch_num = batch_num + 1\n",
    "    \n",
    "    \n",
    "\n",
    "  train_loss = total_loss/len(train_data_loader)\n",
    "  train_bleu = total_bleu/len(train_data_loader)\n",
    "\n",
    "  print('\\ntrain loss:',train_loss.item())\n",
    "  print('train bleu:',train_bleu)\n",
    "  val_loss, val_bleu = eval_model(model, val_data_loader, device)\n",
    "\n",
    "  history['train_loss'].append(train_loss.item())\n",
    "  history['train_bleu'].append(train_bleu)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  history['val_bleu'].append(val_bleu)  \n",
    "    \n",
    "  print('-'*60)\n",
    "  \n",
    "    \n",
    "  savepath = 'model_mask_'+str(epoch+25)\n",
    "  # if val_loss < min_loss:\n",
    "  model.save_pretrained(savepath)\n",
    "  # min_loss = val_loss\n",
    "  \n",
    "  f = open('model_train_record.txt', mode='a')\n",
    "  f.write('EPOCH: '+str(epoch+25)+'\\n'+\n",
    "          'train loss: '+str(train_loss.item())+'\\n'+\n",
    "          'train bleu: '+str(train_bleu)+'\\n'+\n",
    "          'eval loss: '+str(val_loss)+'\\n'+\n",
    "          'eval bleu: '+str(val_bleu)+'\\n')\n",
    "  f.close()\n",
    "\n",
    "\n",
    "print('** Finish **')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bug generation with Transformers, our input will be patched code end with the transfer delimiter. We use beam search to do next token selection, number of beams is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bug(model, patch_ids, tokenizer, device, num_beams):\n",
    "    \n",
    "    output = model.generate(\n",
    "        patch_ids, \n",
    "        do_sample=True, \n",
    "        # top_p=0.95,\n",
    "        # top_k=10,\n",
    "        num_beams=5, \n",
    "        early_stopping=True,\n",
    "        max_length=1025)\n",
    "    \n",
    "    bug_ids = output[0,...,len(patch_ids[0]):-1]\n",
    "\n",
    "    \"\"\"\n",
    "    print('source code: ')\n",
    "    print(tokenizer.decode(patch_ids[0][...,1:-1]))\n",
    "    print('-'*60)\n",
    "    print('bug injection: ')\n",
    "    print(tokenizer.decode(bug_ids))\n",
    "    \"\"\"\n",
    "    return bug_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source code: \n",
      "void sc_osc_handler::handle_message_int_address(ReceivedMessage const& message,\n",
      "         break;\n",
      "     case cmd_cmd:\n",
      "         handle_cmd(message, msg_size, endpoint);\n",
      "         break;\n",
      "     case cmd_version:\n",
      "------------------------------------------------------------\n",
      "bug injection: \n",
      "void sc_handler::handle_handler_address(receivedMessage& message,\n",
      "         break;\n",
      "     case handle_size_size(message, break);\n",
      "     case handle_command_command_version,\n",
      "         break;\n",
      "     case CMD_command_version, handle_version,\n",
      "         break;\n",
      "------------------------------------------------------------\n",
      "actual bug: \n",
      " void sc_osc_handler::handle_message_int_address(ReceivedMessage const& message,\n",
      "         break;\n",
      "     case cmd_cmd:\n",
      "         handle_cmd(message, msg_size, endpoint, 4);\n",
      "         break;\n",
      "     case cmd_version:\n"
     ]
    }
   ],
   "source": [
    "# see one example\n",
    "\n",
    "# data = next(iter(val_data_loader))\n",
    "sample_ids = data['patch_ids'][0].cpu().numpy()\n",
    "sample_ids = torch.from_numpy(np.delete(sample_ids, np.argwhere(sample_ids==1)))\n",
    "sample_ids = torch.unsqueeze(sample_ids, 0).to(device)\n",
    "\n",
    "generate_bug(model, sample_ids, tokenizer, device, 10)\n",
    "\n",
    "actualbug_ids = data['buggy_ids'][0].cpu().numpy()\n",
    "actualbug_ids = torch.from_numpy(np.delete(actualbug_ids, np.argwhere(actualbug_ids==1))) \n",
    "\n",
    "print('-'*60)\n",
    "print('actual bug: ')\n",
    "print(tokenizer.decode(actualbug_ids[...,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(model, data_loader, batch_size, tokenizer, device, num_beams):\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            source_ids = batch['patch_ids'][i].cpu().numpy()\n",
    "            source_ids = torch.from_numpy(np.delete(source_ids, np.argwhere(source_ids==1)))\n",
    "            source_ids = torch.unsqueeze(source_ids, 0).to(device)\n",
    "            \n",
    "            generate_bug(model, source_ids, tokenizer, device, num_beams)\n",
    "            actualbug_ids = batch['buggy_ids'][i].cpu().numpy()\n",
    "            actualbug_ids = torch.from_numpy(np.delete(actualbug_ids, np.argwhere(actualbug_ids==1))) \n",
    "            \n",
    "            print('-'*60)\n",
    "            print('actual bug: ')\n",
    "            print(tokenizer.decode(actualbug_ids[...,:-1]))\n",
    "            print('+'*70)\n",
    "            \n",
    "# generate_model(model, val_data_loader, BATCH_SIZE, tokenizer, device, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the generated bugs, we calculated four values: \n",
    "- BLEU score between the actual buggy code and generated buggy code (BLEU-Model)\n",
    "- BLEU score between actual buggy code and source code (BLEU-Baseline)\n",
    "- The difference between BLEU-Model and BLEU-Baseline (delta)\n",
    "- The proportion of the realistic bugs among all generated bugs\n",
    "\n",
    "A positive delta value indicates: compare to thee patched code, the generated mutant is more similar to the actual bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generate_bleu(model, data_loader, batch_size, tokenizer, device, num_beams):\n",
    "    \n",
    "    total_bleu = 0\n",
    "    total_delta = 0\n",
    "    actualbug_num = 0\n",
    "    batch_num = 1\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        for i in range(batch_size):  # batch_size\n",
    "                source_ids = batch['patch_ids'][i].cpu().numpy()\n",
    "                source_ids = torch.from_numpy(np.delete(source_ids, np.argwhere(source_ids==1)))\n",
    "                source_ids = torch.unsqueeze(source_ids, 0).to(device)\n",
    "                print(source_ids.shape)\n",
    "                \n",
    "                bug_ids = generate_bug(model, source_ids, tokenizer, device ,num_beams)\n",
    "                actualbug_ids = batch['buggy_ids'][i].cpu().numpy()\n",
    "                actualbug_ids = torch.from_numpy(np.delete(actualbug_ids, np.argwhere(actualbug_ids==1)))\n",
    "            \n",
    "                source = tokenizer.convert_ids_to_tokens(source_ids[0][...,1:-1])\n",
    "                bug = tokenizer.convert_ids_to_tokens(bug_ids)\n",
    "                actualbug = tokenizer.convert_ids_to_tokens(actualbug_ids[...,:-1])\n",
    "\n",
    "                smooth = SmoothingFunction()\n",
    "                bleu_inject = sentence_bleu([actualbug], bug, smoothing_function=smooth.method1)\n",
    "                bleu_baseline = sentence_bleu([actualbug], source, smoothing_function=smooth.method1)\n",
    "                bleu_delta = bleu_inject - bleu_baseline\n",
    "                \n",
    "                if bleu_inject==1.0:\n",
    "                    actualbug_num = actualbug_num + 1\n",
    "            \n",
    "                total_bleu += bleu_inject\n",
    "                total_delta += bleu_delta\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        print(\"\\r generating...{:d}/{:d} \".format(batch_num, len(data_loader)), end='',  flush=True)\n",
    "        batch_num = batch_num + 1  \n",
    "        # if batch_num==1001: break\n",
    "        \n",
    "    gen_bleu = total_bleu/(len(data_loader)*batch_size) #len(data_loader)\n",
    "    gen_delta = total_delta/(len(data_loader)*batch_size)\n",
    "    gen_actualbug = actualbug_num/(len(data_loader)*batch_size)\n",
    "  \n",
    "    print('\\ngenerate bleu: ',gen_bleu)\n",
    "    print('generate delta: ',gen_delta)\n",
    "    print('generate actual bugs: ',gen_actualbug)\n",
    "    \n",
    "    f = open('eval_gen.txt', mode='w')\n",
    "    f.write('generate bleu: '+str(gen_bleu)+'\\n'+\n",
    "            'generate delta: '+str(gen_delta)+'\\n'+\n",
    "            'generate actual bugs: '+str(gen_actualbug)+'\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generate_bleu(model, val_data_loader, BATCH_SIZE, tokenizer, device, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
