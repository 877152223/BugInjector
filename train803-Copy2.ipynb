{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/sj3233\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer. Megatron was trained with standard tokenizer(s).\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    vocab_file='/home/sj3233/project21S/nvidia/megatron-gpt2-345m/vocab.json',\n",
    "    merges_file='/home/sj3233/project21S/nvidia/megatron-gpt2-345m/merges.txt')\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"additional_special_tokens\": [\"<to-buggy>\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from $MYDIR/nvidia/megatron-gpt2-345m.\n",
    "directory = '/scratch/sj3233/model_mask_0.10379147529602051_0.09535404294729233'\n",
    "model = GPT2LMHeadModel.from_pretrained(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.35.ln_1.weight\n",
      "transformer.h.35.ln_1.bias\n",
      "transformer.h.35.attn.c_attn.weight\n",
      "transformer.h.35.attn.c_attn.bias\n",
      "transformer.h.35.attn.c_proj.weight\n",
      "transformer.h.35.attn.c_proj.bias\n",
      "transformer.h.35.ln_2.weight\n",
      "transformer.h.35.ln_2.bias\n",
      "transformer.h.35.mlp.c_fc.weight\n",
      "transformer.h.35.mlp.c_fc.bias\n",
      "transformer.h.35.mlp.c_proj.weight\n",
      "transformer.h.35.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "# freeze\n",
    "i = 0\n",
    "for name, param in model.named_parameters():\n",
    "    # print(i)\n",
    "    # print(name)\n",
    "    if i < 422:   # 398 410 422\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)\n",
    "    # print(param.requires_grad)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = '/scratch/sj3233/result1000.csv'\n",
    "df = pd.read_csv(path, sep='@')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BuggyPatchDataset(Dataset):\n",
    "  def __init__(self, buggy_code, patched_code, before, tokenizer, max_len):\n",
    "    self.buggy_code = buggy_code\n",
    "    self.patched_code = patched_code\n",
    "    self.before = before\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.buggy_code)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    buggy = str(self.buggy_code[item])\n",
    "    patched = str(self.patched_code[item])\n",
    "    before = str(self.before[item])\n",
    "    tps = '<s>' + before + patched + '<to-buggy>' + before + buggy + '</s>'\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        tps,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    encoding_p = self.tokenizer.encode_plus(\n",
    "        '<s>' + before + patched + '<to-buggy>',\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    encoding_b = self.tokenizer.encode_plus(\n",
    "        before + buggy + '</s>',\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_tps': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'patch_ids': encoding_p['input_ids'].flatten(),\n",
    "        'buggy_ids': encoding_b['input_ids'].flatten()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180161, 4)\n",
      "(22520, 4)\n",
      "(22521, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, val_test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "df_val, df_test = train_test_split(val_test, train_size=0.5, random_state=42)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def creat_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = BuggyPatchDataset(\n",
    "      buggy_code=df.buggy_code.to_numpy(),\n",
    "      patched_code=df.patched_code.to_numpy(),\n",
    "      before=df.before.to_numpy(),\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=2\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_data_loader = creat_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = creat_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = creat_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_tps', 'attention_mask', 'patch_ids', 'buggy_ids'])\n",
      "torch.Size([8, 1024])\n",
      "torch.Size([8, 1024])\n",
      "torch.Size([8, 1024])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(val_data_loader))\n",
    "\n",
    "print(data.keys())\n",
    "print(data['input_tps'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['buggy_ids'].shape)\n",
    "print(data['buggy_ids'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_ids(out_logits):\n",
    "    pred_ids = torch.argmax(out_logits, dim=2)\n",
    "    return pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mask(input_tps):\n",
    "    tps_np = input_tps.cpu().numpy()\n",
    "    delim_index = np.argwhere(tps_np==52001)\n",
    "    for i in range(len(delim_index)):\n",
    "        tps_np[i,0:delim_index[i,1]] = -100\n",
    "        \n",
    "    return torch.from_numpy(tps_np) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def compute_metrics_bleu(labels, output_ids, tokenizer):\n",
    "    labels_np = labels[..., 1:].cpu().numpy()\n",
    "    output_np = output_ids[..., :-1].cpu().numpy()\n",
    "    tokenizer = tokenizer\n",
    "    \n",
    "    delim_index = np.argwhere(labels_np==52001)\n",
    "    # eos_index = np.argwhere(labels_np==2)\n",
    "    expect_out = []\n",
    "    actual_out = []\n",
    "    \n",
    "    for i in range(len(delim_index)):\n",
    "        expect_out = expect_out + labels_np[i,delim_index[i,1]:].tolist()\n",
    "        actual_out = actual_out + output_np[i,delim_index[i,1]:].tolist()\n",
    "    \n",
    "    expect_out = np.delete(np.array(expect_out), np.argwhere(np.array(expect_out)==1))\n",
    "    actual_out = np.delete(np.array(actual_out), np.argwhere(np.array(actual_out)==1))\n",
    "        \n",
    "    expect_out = torch.from_numpy(expect_out)\n",
    "    actual_out = torch.from_numpy(actual_out)\n",
    "\n",
    "    label_token = tokenizer.convert_ids_to_tokens(expect_out)\n",
    "    output_token = tokenizer.convert_ids_to_tokens(actual_out)\n",
    "\n",
    "    smooth = SmoothingFunction()\n",
    "    bleu_model = sentence_bleu([label_token], output_token, smoothing_function=smooth.method1)\n",
    "\n",
    "    return bleu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_bleu(data['input_tps'][..., :-1], data['input_tps'][..., 1:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_data_loader, device):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_bleu = 0\n",
    "  batch_num = 1\n",
    "\n",
    "  for batch in val_data_loader:\n",
    "\n",
    "    input_tps = batch['input_tps'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = label_mask(input_tps).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tps, attention_mask=attention_mask, labels=labels)\n",
    "    output_ids = get_output_ids(outputs.logits)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss\n",
    "    \n",
    "    bleu_model = compute_metrics_bleu(labels, output_ids, tokenizer)\n",
    "    total_bleu += bleu_model\n",
    "    \n",
    "    print(\"\\r evaluating...{:d}/{:d} \".format(batch_num, len(val_data_loader)), end='',  flush=True)\n",
    "    batch_num = batch_num + 1\n",
    "  \n",
    "  val_loss = total_loss/len(val_data_loader)\n",
    "  val_bleu = total_bleu/len(val_data_loader)\n",
    "  \n",
    "  print('\\neval loss:',val_loss.item())\n",
    "  print('eval bleu:',val_bleu)\n",
    "\n",
    "  return val_loss.item(), val_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Start **\n",
      "EPOCH: 1/10 \n",
      " training...12106/22521 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from collections import defaultdict\n",
    "\n",
    "EPOCHS = 10\n",
    "history = defaultdict(list)\n",
    "min_loss = 100\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=2e-5,  \n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-08,\n",
    "    weight_decay=0.01)  \n",
    "\n",
    "print('** Start **')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  total_loss = 0\n",
    "  total_bleu = 0\n",
    "  batch_num = 1\n",
    "  print('EPOCH: %d/%d '%(epoch+1, EPOCHS))\n",
    "\n",
    "  for batch in train_data_loader:\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    input_tps = batch['input_tps'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = label_mask(input_tps).to(device)\n",
    "\n",
    "    outputs = model(input_tps, attention_mask=attention_mask, labels=labels)\n",
    "    output_ids = get_output_ids(outputs.logits)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    total_loss += loss\n",
    "    \n",
    "    bleu_model = compute_metrics_bleu(labels, output_ids, tokenizer)\n",
    "    total_bleu += bleu_model\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    print(\"\\r training...{:d}/{:d} \".format(batch_num, len(train_data_loader)), end='',  flush=True)\n",
    "    batch_num = batch_num + 1\n",
    "    \n",
    "    \n",
    "\n",
    "  train_loss = total_loss/len(train_data_loader)\n",
    "  train_bleu = total_bleu/len(train_data_loader)\n",
    "\n",
    "  print('\\ntrain loss:',train_loss.item())\n",
    "  print('train bleu:',train_bleu)\n",
    "  val_loss, val_bleu = eval_model(model, val_data_loader, device)\n",
    "\n",
    "  history['train_loss'].append(train_loss.item())\n",
    "  history['train_bleu'].append(train_bleu)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  history['val_bleu'].append(val_bleu)  \n",
    "    \n",
    "  print('-'*60)\n",
    "  \n",
    " \n",
    "  savepath = 'model_mask_best'\n",
    "  if val_loss < min_loss:\n",
    "    model.save_pretrained(savepath)\n",
    "    min_loss = val_loss\n",
    "\n",
    "\n",
    "print('** Finish **')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savepath = 'model_masktry'+str(epoch+1)+'_'+str(train_loss.item())+'_'+str(val_loss)\n",
    "# model.save_pretrained(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bug(model, patch_ids, tokenizer, device):\n",
    "    model.eval()\n",
    "    next_ids = patch_ids\n",
    "    for _ in range(512):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=next_ids.to(device))\n",
    "            next_logits = torch.argmax(outputs.logits, dim=1)[...,-1].item()\n",
    "            next_ids = torch.from_numpy(np.hstack((next_ids.cpu().numpy(), next_logits)))\n",
    "        if next_logits == 2:\n",
    "            break\n",
    "\n",
    "    bug_ids = next_ids[...,len(patch_ids):]\n",
    "    \n",
    "    print('source code: ')\n",
    "    print(tokenizer.decode(patch_ids[...,1:-1]))\n",
    "    print('-'*60)\n",
    "    print('bug injection: ')\n",
    "    print(tokenizer.decode(bug_ids[...,:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "sample_ids = data['patch_ids'][1].cpu().numpy()\n",
    "sample_ids = torch.from_numpy(np.delete(sample_ids, np.argwhere(sample_ids==1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source code: \n",
      "void sc_osc_handler::handle_message_int_address(ReceivedMessage const& message,\n",
      "         break;\n",
      "     case cmd_cmd:\n",
      "         handle_cmd(message, msg_size, endpoint);\n",
      "         break;\n",
      "     case cmd_version:\n",
      "------------------------------------------------------------\n",
      "bug injection: \n",
      "void sc_handler::handle_handler_address(receivedMessage& message,\n",
      "         break;\n",
      "     case handle_size_size(message, break);\n",
      "     case handle_command_command_version,\n",
      "         break;\n",
      "     case CMD_command_version, handle_version,\n",
      "         break;\n",
      "------------------------------------------------------------\n",
      "actual bug: \n",
      " void sc_osc_handler::handle_message_int_address(ReceivedMessage const& message,\n",
      "         break;\n",
      "     case cmd_cmd:\n",
      "         handle_cmd(message, msg_size, endpoint, 4);\n",
      "         break;\n",
      "     case cmd_version:\n"
     ]
    }
   ],
   "source": [
    "generate_bug(model, sample_ids, tokenizer, device)\n",
    "actualbug_ids = data['buggy_ids'][1].cpu().numpy()\n",
    "actualbug_ids = torch.from_numpy(np.delete(actualbug_ids, np.argwhere(actualbug_ids==1))) \n",
    "print('-'*60)\n",
    "print('actual bug: ')\n",
    "print(tokenizer.decode(actualbug_ids[...,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data_loader, batch_size, tokrnizer, device):\n",
    "    \n",
    "    for batch in test_data_loader:\n",
    "        for i in range(batch_size):\n",
    "            source_ids = batch['patch_ids'][i].cpu().numpy()\n",
    "            source_ids = torch.from_numpy(np.delete(source_ids, np.argwhere(source_ids==1)))\n",
    "            generate_bug(model, source_ids, tokenizer, device)\n",
    "            actualbug_ids = batch['buggy_ids'][i].cpu().numpy()\n",
    "            actualbug_ids = torch.from_numpy(np.delete(actualbug_ids, np.argwhere(actualbug_ids==1))) \n",
    "            print('-'*60)\n",
    "            print('actual bug: ')\n",
    "            print(tokenizer.decode(actualbug_ids[...,:-1]))\n",
    "            print('+'*70)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, val_data_loader, BATCH_SIZE, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
